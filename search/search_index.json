{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FastEnhancer github | paper . Contents: Installation Installation for all features Minimal installation for training Minimal installation for calculating objective metrics Minimal installation for ONNX exporting Minimal installation for ONNXRuntime (spec2spec version) Minimal installation for ONNXRuntime (wav2wav version) Training Training code Training recipes FastEnhancer BSRNN FSPEN LiSenNet Experience sharing Calculating objevtice metrics ONNXRuntime Exporting to ONNX and executing ONNXRuntime Executing ONNXRuntime","title":"Home"},{"location":"#fastenhancer","text":"github | paper .","title":"FastEnhancer"},{"location":"#contents","text":"Installation Installation for all features Minimal installation for training Minimal installation for calculating objective metrics Minimal installation for ONNX exporting Minimal installation for ONNXRuntime (spec2spec version) Minimal installation for ONNXRuntime (wav2wav version) Training Training code Training recipes FastEnhancer BSRNN FSPEN LiSenNet Experience sharing Calculating objevtice metrics ONNXRuntime Exporting to ONNX and executing ONNXRuntime Executing ONNXRuntime","title":"Contents:"},{"location":"metrics/","text":"Objective Metrics We support - DNSMOS P.808 and P.835 (SIG, BAK, OVL) - SCOREQ ( github ) - It provides two domains: Natural Speech domain trained for real recordings, and Synthetic Speech domain trained for TTS. - We use Natural Speech domain. - It provides two modes: no-reference (NR) and non-matching reference (NMR). NR mode is a non-intrusive mode. NMR mode takes reference and degraded signals. However, the reference doesn't have to be the clean speech pair of the degraded signal. - We use full reference mode, meaning clean speech is given as the reference of the NMR mode. - SISDR - On some implementations, mean values of reference and degraded signal are substracted. However, we don't perform mean substraction. - PESQ ( github ) - We use P.862.2 without Corrigendum 2. For more information, see the following paper and github . - STOI and ESTOI ( github ) - WER - Whisper-large-v3-turbo is used. - For Voicebank-Demand testset, there exists transcript, so we can calculate WER. - For DNS-Challenge dev testset (Interspeech2020, synthetic), transcript is not provided. Furthermore, each clean speech is a random crop of a long audio, so the beginning and end of the speech are cut off. Therefore, it is impossible to create a transcript. We don't calculate WER for DNS-Challenge. Calculating objective metrics Assume you want to calculate objective metrics for a model in logs/fastenhancer_l . For Voicebank-Demand, run the following code: CUDA_VISIBLE_DEVICES=0 python -m scripts.metrics_ns -n fastenhancer_l --transcript-dir PATH-TO-TRANSCRIPT For DNS-Challenge, run the following code: CUDA_VISIBLE_DEVICES=0 python -m scripts.metrics_ns -n fastenhancer_l --wer False If you want to use cuda:1, don't set -d cuda:1 . Set CUDA_VISIBLE_DEVICES=1 and -d cuda:0 .","title":"Objective Metrics"},{"location":"metrics/#objective-metrics","text":"We support - DNSMOS P.808 and P.835 (SIG, BAK, OVL) - SCOREQ ( github ) - It provides two domains: Natural Speech domain trained for real recordings, and Synthetic Speech domain trained for TTS. - We use Natural Speech domain. - It provides two modes: no-reference (NR) and non-matching reference (NMR). NR mode is a non-intrusive mode. NMR mode takes reference and degraded signals. However, the reference doesn't have to be the clean speech pair of the degraded signal. - We use full reference mode, meaning clean speech is given as the reference of the NMR mode. - SISDR - On some implementations, mean values of reference and degraded signal are substracted. However, we don't perform mean substraction. - PESQ ( github ) - We use P.862.2 without Corrigendum 2. For more information, see the following paper and github . - STOI and ESTOI ( github ) - WER - Whisper-large-v3-turbo is used. - For Voicebank-Demand testset, there exists transcript, so we can calculate WER. - For DNS-Challenge dev testset (Interspeech2020, synthetic), transcript is not provided. Furthermore, each clean speech is a random crop of a long audio, so the beginning and end of the speech are cut off. Therefore, it is impossible to create a transcript. We don't calculate WER for DNS-Challenge.","title":"Objective Metrics"},{"location":"metrics/#calculating-objective-metrics","text":"Assume you want to calculate objective metrics for a model in logs/fastenhancer_l . For Voicebank-Demand, run the following code: CUDA_VISIBLE_DEVICES=0 python -m scripts.metrics_ns -n fastenhancer_l --transcript-dir PATH-TO-TRANSCRIPT For DNS-Challenge, run the following code: CUDA_VISIBLE_DEVICES=0 python -m scripts.metrics_ns -n fastenhancer_l --wer False If you want to use cuda:1, don't set -d cuda:1 . Set CUDA_VISIBLE_DEVICES=1 and -d cuda:0 .","title":"Calculating objective metrics"},{"location":"onnx/","text":"ONNX You can export your own model to ONNX and execute ONNXRuntime. You can also download pre-compiled ONNX file and execute ONNXRuntime. There are two ways for ONNXRuntime. Spectrogram-to-spectrogram (spec2spec) version: STFT and iSTFT are done in PyTorch. Only the neural network part is calculated in ONNXRuntime. This version is exported using torch.export. Waveform-to-waveform (wav2wav) version: STFT and iSTFT are also done in ONNXRuntime. This version is exported using torchdynamo. Only FastEnhancers are successfully exported to this version. For other models, it is either impossible to create a wav2wav version or the ONNXRuntime execution speed is very slow. The RTFs in our paper are measured using spec2spec versions. Exporting to ONNX and executing ONNXRuntime Suppose you have trained a model and saved its checkpoints at logs/fastenhancer_l . The code below exports the model to ONNX, saves to onnx/fastenhancer_l.onnx , executes the ONNXRuntime, and calculates the RTF. Wav2wav version: python -m scripts.export_onnx -n fastenhancer_l --onnx-path onnx/fastenhancer_l.onnx Spec2spec version: python -m scripts.export_onnx_spec -n fastenhancer_l --onnx-path onnx/fastenhancer_l.onnx Executing ONNXRuntime You can download a pre-compiled ONNX file from here . If you downloaded a wav2wav version in onnx/fastenhancer_t.onnx , run the following code: python -m scripts.test_onnx --onnx-path onnx/fastenhancer_t.onnx If you downloaded a spec2spec version in onnx/fastenhancer_t_spec.onnx , run the following code: python -m scripts.test_onnx_spec --onnx-path onnx/fastenhancer_t_spec.onnx There are some model-specific settings. - For FastEnhancer-M, you should set --hop-size 160 : python -m scripts.test_onnx --onnx-path onnx/fastenhancer_m.onnx --hop-size 160 For FastEnhancer-L, you should set --hop-size 100 : python -m scripts.test_onnx --onnx-path onnx/fastenhancer_l.onnx --hop-size 100 For GTCRN, you should set --win-type hann-sqrt : python -m scripts.test_onnx_spec --onnx-path onnx/gtcrn_spec.onnx --win-type hann-sqrt For other models, use default settings. More information about wav2wav version Let x denote a noisy input and y denote an enhanced signal. Let n denote an fft size and h denote a hop size. In the wav2wav version, at every i -th iteration, the model gets x[i*h+(n-h):i*h+n] as an input and returns y[i*h:(i+1)*h] . This means that the input and the output has a delay of n-h . Why? Obviously, At the first iteration, the model takes x[0:n] and generates an enhanced signal y[0:n] . At the second iteration, the model takes x[h:n+h] and generates an enhanced signal y[h:n+h] which is overlap-added to the previous iteration's output. However, This implies that at the end of first iteration, y[0:h] is completed while y[h:n] isn't. Also, at the beggining of second iteration, only x[n:n+h] is a new input. x[h:n] was already given at the first iteration. So, At the first iteration, the model saves x[h:n] as its input cache and y[h:n] as its output cache. At the second iteration, the model gets x[n:n+h] as an input. The model concatenate its input cache with the new input to make x[h:n+h] . The model generates y[h:n+h] . It is overlap-added with the previous output cache y[h:n] . Since y[h:2*h] is now completed, it is returned. The model caches x[2*h:n+h] and y[2*h:n+h] for the next iteration. The final algorithm is as below: Initially, the model has an input cache cache_in whose length is n-h and filled with zeros. The model also has an output cache cache_out whose length is n-h and filled with zeros. At every iterations, the model gets a new input chunk x with a length of h . The model concatenate the input chunk with its input cache to create an input with a length of n : x = torch.cat([cache_in, x]) The model saves the last n-h samples as its new input cache: cache_in = x[h:n] The model generates an enhanced signal y with a length of n : y = model(x) The model performs an overlap-add: y[0:n-h] += cache_out The model saves the last n-h samples as its new output cache: cache_out = y[h:n] The model returns the first h samples: return y[0:h] Outside the model, it seems that the model gets an input with a length of h and returns an output with a length of h . However, you now understand that those input and output are not time-aligned. They have a time difference of n-h .","title":"ONNX"},{"location":"onnx/#onnx","text":"You can export your own model to ONNX and execute ONNXRuntime. You can also download pre-compiled ONNX file and execute ONNXRuntime. There are two ways for ONNXRuntime. Spectrogram-to-spectrogram (spec2spec) version: STFT and iSTFT are done in PyTorch. Only the neural network part is calculated in ONNXRuntime. This version is exported using torch.export. Waveform-to-waveform (wav2wav) version: STFT and iSTFT are also done in ONNXRuntime. This version is exported using torchdynamo. Only FastEnhancers are successfully exported to this version. For other models, it is either impossible to create a wav2wav version or the ONNXRuntime execution speed is very slow. The RTFs in our paper are measured using spec2spec versions.","title":"ONNX"},{"location":"onnx/#exporting-to-onnx-and-executing-onnxruntime","text":"Suppose you have trained a model and saved its checkpoints at logs/fastenhancer_l . The code below exports the model to ONNX, saves to onnx/fastenhancer_l.onnx , executes the ONNXRuntime, and calculates the RTF. Wav2wav version: python -m scripts.export_onnx -n fastenhancer_l --onnx-path onnx/fastenhancer_l.onnx Spec2spec version: python -m scripts.export_onnx_spec -n fastenhancer_l --onnx-path onnx/fastenhancer_l.onnx","title":"Exporting to ONNX and executing ONNXRuntime"},{"location":"onnx/#executing-onnxruntime","text":"You can download a pre-compiled ONNX file from here . If you downloaded a wav2wav version in onnx/fastenhancer_t.onnx , run the following code: python -m scripts.test_onnx --onnx-path onnx/fastenhancer_t.onnx If you downloaded a spec2spec version in onnx/fastenhancer_t_spec.onnx , run the following code: python -m scripts.test_onnx_spec --onnx-path onnx/fastenhancer_t_spec.onnx There are some model-specific settings. - For FastEnhancer-M, you should set --hop-size 160 : python -m scripts.test_onnx --onnx-path onnx/fastenhancer_m.onnx --hop-size 160 For FastEnhancer-L, you should set --hop-size 100 : python -m scripts.test_onnx --onnx-path onnx/fastenhancer_l.onnx --hop-size 100 For GTCRN, you should set --win-type hann-sqrt : python -m scripts.test_onnx_spec --onnx-path onnx/gtcrn_spec.onnx --win-type hann-sqrt For other models, use default settings.","title":"Executing ONNXRuntime"},{"location":"onnx/#more-information-about-wav2wav-version","text":"Let x denote a noisy input and y denote an enhanced signal. Let n denote an fft size and h denote a hop size. In the wav2wav version, at every i -th iteration, the model gets x[i*h+(n-h):i*h+n] as an input and returns y[i*h:(i+1)*h] . This means that the input and the output has a delay of n-h . Why? Obviously, At the first iteration, the model takes x[0:n] and generates an enhanced signal y[0:n] . At the second iteration, the model takes x[h:n+h] and generates an enhanced signal y[h:n+h] which is overlap-added to the previous iteration's output. However, This implies that at the end of first iteration, y[0:h] is completed while y[h:n] isn't. Also, at the beggining of second iteration, only x[n:n+h] is a new input. x[h:n] was already given at the first iteration. So, At the first iteration, the model saves x[h:n] as its input cache and y[h:n] as its output cache. At the second iteration, the model gets x[n:n+h] as an input. The model concatenate its input cache with the new input to make x[h:n+h] . The model generates y[h:n+h] . It is overlap-added with the previous output cache y[h:n] . Since y[h:2*h] is now completed, it is returned. The model caches x[2*h:n+h] and y[2*h:n+h] for the next iteration. The final algorithm is as below: Initially, the model has an input cache cache_in whose length is n-h and filled with zeros. The model also has an output cache cache_out whose length is n-h and filled with zeros. At every iterations, the model gets a new input chunk x with a length of h . The model concatenate the input chunk with its input cache to create an input with a length of n : x = torch.cat([cache_in, x]) The model saves the last n-h samples as its new input cache: cache_in = x[h:n] The model generates an enhanced signal y with a length of n : y = model(x) The model performs an overlap-add: y[0:n-h] += cache_out The model saves the last n-h samples as its new output cache: cache_out = y[h:n] The model returns the first h samples: return y[0:h] Outside the model, it seems that the model gets an input with a length of h and returns an output with a length of h . However, you now understand that those input and output are not time-aligned. They have a time difference of n-h .","title":"More information about wav2wav version"},{"location":"dataset/","text":"Dataset This section describes about dataset preparation. For training, we need four types of dataset for: - Training - Validation - Inferencing (During training, every infer.interval epochs, we perform inference for a small amount of data and write the tensorboard log). - Calculating objective metrics (We re-use valid set). Supported Datasets Voicebank-Demand DNSChallenge","title":"Overview"},{"location":"dataset/#dataset","text":"This section describes about dataset preparation. For training, we need four types of dataset for: - Training - Validation - Inferencing (During training, every infer.interval epochs, we perform inference for a small amount of data and write the tensorboard log). - Calculating objective metrics (We re-use valid set).","title":"Dataset"},{"location":"dataset/#supported-datasets","text":"Voicebank-Demand DNSChallenge","title":"Supported Datasets"},{"location":"dataset/dns-challenge/","text":"DNS-Challenge Datasets DNS-Challenge datasets are noise suppression dataset. DNS-Challenge 1 (Interspeech 2020): Widebank(16kHz), 500 hours of clean speech from LibriVox audiobooks. It contains a synthetic dev test set with clean and noisy pairs that can be used for calculating intrusive objective metrics. It also contains a real recording dev test set and a blind test set that can be used for calculating non-intrusive objective metrics and subjective metrics. DNS-Challenge 2 (ICASSP 2021): Wideband(16kHz), 760 hours of clean speech (singing voice, emotion data, and non-English are added). DNS-Challenge 3 (Interspeech 2021): Wideband(16kHz) & Fullband(48kHz). DNS-Challenge 4 (ICASSP 2022): Fullband(48kHz). Personalized train dataset added. DNS-Challenge 5 (ICASSP 2023): Fullband(48kHz). Headset dataset added. For 16kHz dataset, we recommend to download DNS-Challenge 3 wideband train dataset and DNS-Challenge 1 synthetic testset. For 48kHz dataset, we recommend to download DNS-Challenge 5 speakerphone train dataset. Note that no test set with clean and noisy pairs is provided. Test sets with only noisy files are provided. Preparing dataset Download Download the dataset from here . For DNS-Challenge 1 synthetic dev test set, we provide a pre-processed version here Downsample If needed, downsample the dataset using scripts/resample.py . For example, if you want to downsample to 24kHz, run the code below: python -m scripts.resample --to-sr 24000 --from-dir ~/Datasets/DNS_Challenge/dataset_fullband --to-dir ~/Datasets/DNS_Challenge/dataset_24khz Modify Configuration file You have to change the dataset path and sampling rate of configuration file. Since there's no official testset composed of clean and noisy pairs for fullband, you can train with DNS-Challenge dataset and validate with other dataset. For example, in configs/fastenhancer/huge_dns.yaml , we trained FastEnhancer-Huge-noncausal at 24kHz using DNS-Challenge dataset and validated using Voicebank-demand testset. Modify various configurations in data section of the config file as you wish. If the dataset loading is too slow, you may consider increasing train.num_workers . If the speed is still slow, we recommend to write a code to synthesize dataset in advance and train using the synthesized dataset. Dataset Code For DNS-Challenge dataset, we load clean speech, noise, and optionally RIR. They are later mixed on-the-fly at the training code. To check or modify the dataset code, see utils/data/ns_on_the_fly.py . Training Code To check or modify the training code for DNS-Challenge, see wrappers/ns_on_the_fly.py . Clean speech and noise pairs are loaded and mixed on-the-fly to generate noisy. If you want to add RIR to clean speech, you have to modify the code. config.yaml Set data.reverb_prob higher than 0 and leq than 1. Set data.rir_length to the max length of rir. For DNS-Challenge synthetic RIR, set it to 2 seconds (32000 for 16kHz and 96000 for 48khz). wrappers/ns_on_the_fly.py Add rir to self.keys Load batch['rir'] , and give it to self.snr_mixer . In this case, the rir-convolved clean speech becomes the target, which means your model doen't perform dereverberation. If you want to do dereverberation along with noise suppression, you have to implement on your own. Some papers preserve only the first 100ms reflections and use that as a target (GTCRN, UL-UNAS). In URGENT challenges, they find the rir_start_index and preserve the 50ms reflections from that starting point.","title":"DNS-Challenge"},{"location":"dataset/dns-challenge/#dns-challenge-datasets","text":"DNS-Challenge datasets are noise suppression dataset. DNS-Challenge 1 (Interspeech 2020): Widebank(16kHz), 500 hours of clean speech from LibriVox audiobooks. It contains a synthetic dev test set with clean and noisy pairs that can be used for calculating intrusive objective metrics. It also contains a real recording dev test set and a blind test set that can be used for calculating non-intrusive objective metrics and subjective metrics. DNS-Challenge 2 (ICASSP 2021): Wideband(16kHz), 760 hours of clean speech (singing voice, emotion data, and non-English are added). DNS-Challenge 3 (Interspeech 2021): Wideband(16kHz) & Fullband(48kHz). DNS-Challenge 4 (ICASSP 2022): Fullband(48kHz). Personalized train dataset added. DNS-Challenge 5 (ICASSP 2023): Fullband(48kHz). Headset dataset added. For 16kHz dataset, we recommend to download DNS-Challenge 3 wideband train dataset and DNS-Challenge 1 synthetic testset. For 48kHz dataset, we recommend to download DNS-Challenge 5 speakerphone train dataset. Note that no test set with clean and noisy pairs is provided. Test sets with only noisy files are provided.","title":"DNS-Challenge Datasets"},{"location":"dataset/dns-challenge/#preparing-dataset","text":"","title":"Preparing dataset"},{"location":"dataset/dns-challenge/#download","text":"Download the dataset from here . For DNS-Challenge 1 synthetic dev test set, we provide a pre-processed version here","title":"Download"},{"location":"dataset/dns-challenge/#downsample","text":"If needed, downsample the dataset using scripts/resample.py . For example, if you want to downsample to 24kHz, run the code below: python -m scripts.resample --to-sr 24000 --from-dir ~/Datasets/DNS_Challenge/dataset_fullband --to-dir ~/Datasets/DNS_Challenge/dataset_24khz","title":"Downsample"},{"location":"dataset/dns-challenge/#modify-configuration-file","text":"You have to change the dataset path and sampling rate of configuration file. Since there's no official testset composed of clean and noisy pairs for fullband, you can train with DNS-Challenge dataset and validate with other dataset. For example, in configs/fastenhancer/huge_dns.yaml , we trained FastEnhancer-Huge-noncausal at 24kHz using DNS-Challenge dataset and validated using Voicebank-demand testset. Modify various configurations in data section of the config file as you wish. If the dataset loading is too slow, you may consider increasing train.num_workers . If the speed is still slow, we recommend to write a code to synthesize dataset in advance and train using the synthesized dataset.","title":"Modify Configuration file"},{"location":"dataset/dns-challenge/#dataset-code","text":"For DNS-Challenge dataset, we load clean speech, noise, and optionally RIR. They are later mixed on-the-fly at the training code. To check or modify the dataset code, see utils/data/ns_on_the_fly.py .","title":"Dataset Code"},{"location":"dataset/dns-challenge/#training-code","text":"To check or modify the training code for DNS-Challenge, see wrappers/ns_on_the_fly.py . Clean speech and noise pairs are loaded and mixed on-the-fly to generate noisy. If you want to add RIR to clean speech, you have to modify the code. config.yaml Set data.reverb_prob higher than 0 and leq than 1. Set data.rir_length to the max length of rir. For DNS-Challenge synthetic RIR, set it to 2 seconds (32000 for 16kHz and 96000 for 48khz). wrappers/ns_on_the_fly.py Add rir to self.keys Load batch['rir'] , and give it to self.snr_mixer . In this case, the rir-convolved clean speech becomes the target, which means your model doen't perform dereverberation. If you want to do dereverberation along with noise suppression, you have to implement on your own. Some papers preserve only the first 100ms reflections and use that as a target (GTCRN, UL-UNAS). In URGENT challenges, they find the rir_start_index and preserve the 50ms reflections from that starting point.","title":"Training Code"},{"location":"dataset/voicebank-demand/","text":"Voicebank-Demand Dataset Voicebank-Demand, also known as VCTK-Demand, is a noise suppression dataset with a sampling rate of 48kHz. There are two train datasets: one is a 28-speaker version, and the other is a 56-speaker version. In many papers, including ours, the 28-speaker version is used. Preparing dataset Download Download the train data, test data, and logfiles from here . Download a trainscript file of the testset from here . Downsample If needed, downsample the dataset using scripts/resample.py . For example, if you want to downsample to 16kHz, run the code below: python -m scripts.resample --to-sr 16000 --from-dir ~/Datasets/voicebank-demand/48k --to-dir ~/Datasets/voicebank-demand/16k After downloading, the directory may look like this: voicebank-demand \u251c\u2500 16k | \u251c\u2500 clean_testset_wav | \u251c\u2500 clean_trainset_28spk_wav | \u251c\u2500 noisy_testset_wav | \u2514\u2500 noisy_trainset_28spk_wav \u251c\u2500 48k | \u251c\u2500 clean_testset_wav | \u251c\u2500 clean_trainset_28spk_wav | \u251c\u2500 noisy_testset_wav | \u2514\u2500 noisy_trainset_28spk_wav \u2514\u2500 logfiles \u251c\u2500 log_readme.txt \u251c\u2500 log_testset.txt \u251c\u2500 log_trainset_28spk.txt \u2514\u2500 transcript_testset.txt Modify Configuration file You have to change the dataset path and sampling rate of configuration file. For example, to train FastEnhancer-B, change data section in configs/fastenhancer/b.yaml . Dataset Code For Voicebank-demand dataset, we load clean and noisy speech pairs. To check or modify the dataset code, see utils/data/voicebank_demand.py . Training Code To check or modify the training code for Voicebank-Demand, see wrappers/ns.py . Clean and noisy pairs are loaded for training.","title":"Voicebank-Demand"},{"location":"dataset/voicebank-demand/#voicebank-demand-dataset","text":"Voicebank-Demand, also known as VCTK-Demand, is a noise suppression dataset with a sampling rate of 48kHz. There are two train datasets: one is a 28-speaker version, and the other is a 56-speaker version. In many papers, including ours, the 28-speaker version is used.","title":"Voicebank-Demand Dataset"},{"location":"dataset/voicebank-demand/#preparing-dataset","text":"","title":"Preparing dataset"},{"location":"dataset/voicebank-demand/#download","text":"Download the train data, test data, and logfiles from here . Download a trainscript file of the testset from here .","title":"Download"},{"location":"dataset/voicebank-demand/#downsample","text":"If needed, downsample the dataset using scripts/resample.py . For example, if you want to downsample to 16kHz, run the code below: python -m scripts.resample --to-sr 16000 --from-dir ~/Datasets/voicebank-demand/48k --to-dir ~/Datasets/voicebank-demand/16k After downloading, the directory may look like this: voicebank-demand \u251c\u2500 16k | \u251c\u2500 clean_testset_wav | \u251c\u2500 clean_trainset_28spk_wav | \u251c\u2500 noisy_testset_wav | \u2514\u2500 noisy_trainset_28spk_wav \u251c\u2500 48k | \u251c\u2500 clean_testset_wav | \u251c\u2500 clean_trainset_28spk_wav | \u251c\u2500 noisy_testset_wav | \u2514\u2500 noisy_trainset_28spk_wav \u2514\u2500 logfiles \u251c\u2500 log_readme.txt \u251c\u2500 log_testset.txt \u251c\u2500 log_trainset_28spk.txt \u2514\u2500 transcript_testset.txt","title":"Downsample"},{"location":"dataset/voicebank-demand/#modify-configuration-file","text":"You have to change the dataset path and sampling rate of configuration file. For example, to train FastEnhancer-B, change data section in configs/fastenhancer/b.yaml .","title":"Modify Configuration file"},{"location":"dataset/voicebank-demand/#dataset-code","text":"For Voicebank-demand dataset, we load clean and noisy speech pairs. To check or modify the dataset code, see utils/data/voicebank_demand.py .","title":"Dataset Code"},{"location":"dataset/voicebank-demand/#training-code","text":"To check or modify the training code for Voicebank-Demand, see wrappers/ns.py . Clean and noisy pairs are loaded for training.","title":"Training Code"},{"location":"installation/","text":"Installation Installation for all features First, follow Installation for Training . Second, install the following packages: pip install torchmetrics jiwer onnx onnxsim onnxscript pip install git+https://github.com/openai/whisper.git Third, install onnxruntime-gpu . Make sure to install the version that matches your CUDA version. If you cannot install the GPU version, you can install the CPU version instead. However, DNSMOS and SCOREQ will run on CPU and the metrics_ns.py code will run very slow. Minimal installation for training Required by train.py and train_torchrun.py . Refer to Installation for Training . Minimal installation for calculating objective metrics Required by metrics_ns.py . First, follow Installation for Training . Second, install the following pacakges: pip install torchmetrics jiwer pip install git+https://github.com/openai/whisper.git Finally, install onnxruntime-gpu . Make sure to install the version that matches your CUDA version. If you cannot install the GPU version, you can install the CPU version instead. However, DNSMOS and SCOREQ will run on CPU and the metrics_ns.py code will run very slow. Minimal installation for ONNX exporting Required by scripts/export_onnx.py and scripts/export_onnx_spec.py . First, follow Installation for Training . Second, install the following pacakges: pip install onnx onnxsim onnxscript Minimal Installation for ONNXRuntime (spec2spec version) Required by scripts/test_onnx_spec.py . First, install PyTorch . It doesn't need to be GPU version. Second, install the following pacakges: pip install numpy scipy librosa tqdm Finally, install install onnxruntime . It doesn't matter whether you intsall a CPU version or a GPU version. Even if you install a GPU version, the code will run on CPU anyway. Minimal Installation for ONNXRuntime (wav2wav version) Required by scripts/test_onnx.py . You don't need to install PyTorch in this case. First, install the following pacakges: pip install numpy scipy librosa tqdm Then install install onnxruntime . It doesn't matter whether you intsall a CPU version or a GPU version. Even if you install a GPU version, the code will run on CPU anyway.","title":"Overview"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#installation-for-all-features","text":"First, follow Installation for Training . Second, install the following packages: pip install torchmetrics jiwer onnx onnxsim onnxscript pip install git+https://github.com/openai/whisper.git Third, install onnxruntime-gpu . Make sure to install the version that matches your CUDA version. If you cannot install the GPU version, you can install the CPU version instead. However, DNSMOS and SCOREQ will run on CPU and the metrics_ns.py code will run very slow.","title":"Installation for all features"},{"location":"installation/#minimal-installation-for-training","text":"Required by train.py and train_torchrun.py . Refer to Installation for Training .","title":"Minimal installation for training"},{"location":"installation/#minimal-installation-for-calculating-objective-metrics","text":"Required by metrics_ns.py . First, follow Installation for Training . Second, install the following pacakges: pip install torchmetrics jiwer pip install git+https://github.com/openai/whisper.git Finally, install onnxruntime-gpu . Make sure to install the version that matches your CUDA version. If you cannot install the GPU version, you can install the CPU version instead. However, DNSMOS and SCOREQ will run on CPU and the metrics_ns.py code will run very slow.","title":"Minimal installation for calculating objective metrics"},{"location":"installation/#minimal-installation-for-onnx-exporting","text":"Required by scripts/export_onnx.py and scripts/export_onnx_spec.py . First, follow Installation for Training . Second, install the following pacakges: pip install onnx onnxsim onnxscript","title":"Minimal installation for ONNX exporting"},{"location":"installation/#minimal-installation-for-onnxruntime-spec2spec-version","text":"Required by scripts/test_onnx_spec.py . First, install PyTorch . It doesn't need to be GPU version. Second, install the following pacakges: pip install numpy scipy librosa tqdm Finally, install install onnxruntime . It doesn't matter whether you intsall a CPU version or a GPU version. Even if you install a GPU version, the code will run on CPU anyway.","title":"Minimal Installation for ONNXRuntime (spec2spec version)"},{"location":"installation/#minimal-installation-for-onnxruntime-wav2wav-version","text":"Required by scripts/test_onnx.py . You don't need to install PyTorch in this case. First, install the following pacakges: pip install numpy scipy librosa tqdm Then install install onnxruntime . It doesn't matter whether you intsall a CPU version or a GPU version. Even if you install a GPU version, the code will run on CPU anyway.","title":"Minimal Installation for ONNXRuntime (wav2wav version)"},{"location":"installation/training/","text":"Installation for Training We tested under: - PyTorch==2.7.1, CudaToolkit==11.8, Python==3.13 - PyTorch==2.7.1, CudaToolkit==12.8, Python==3.13 Note that we failed under PyTorch==2.8.0. - We succeeded to train, calculate metrics, export ONNX spec2spec, and execute ONNXRuntime. - However, we failed to export ONNX wav2wav. After downgrading to PyTorch==2.7.1, the problem is solved. (0) Decide Python, CUDA toolkit, and PyTorch versions Before install, you have to decide which version to install (including Python, CUDA toolkit, and PyTorch). Note that PyTorch>=2.3 is recommended. On PyTorch<2.3 , torch.nn.utils.parametrizations.weight_norm is not implemented, so you have to change the codes and .yaml files. You also have to remove device_id argument of dist.init_process_group in train.py . First, check CUDA toolkit versions that your nvidia driver supports: nvidia-smi | grep \"CUDA Version\" The output should look like this: | NVIDIA-SMI 580.65.06 Driver Version: 580.65.06 CUDA Version: 13.0 | That is the maximum CUDA toolkit version you can install. In our case, we can choose any version <= 13.0 . Second, visit here and decide PyTorch, Python, and CUDA toolkit version. Then install Python to your environment. For the rest of this document, we will use torch-2.7.1+cu128-cp313 version, meaning PyTorch 2.7.1 , CUDA toolkit 12.8 , and Python 3.13 . You can use your favorite environment manager. We use miniconda as below: conda create -n fastenhancer python=3.13 -c conda-forge conda activate fastenhancer (1) Install CUDA toolkit and cuDNN Download a local runfile of CUDA toolkit and install. In the following example, we will install CUDA toolkit 12.8 in /home/shahn/.local/cuda-12.8 : wget https://developer.download.nvidia.com/compute/cuda/12.8.1/local_installers/cuda_12.8.1_570.124.06_linux.run chmod +x cuda_12.8.1_570.124.06_linux.run ./cuda_12.8.1_570.124.06_linux.run \\ --silent \\ --toolkit \\ --installpath=/home/shahn/.local/cuda-12.8 \\ --no-opengl-libs \\ --no-drm \\ --no-man-page Then, install a tar file of cuDNN for your CUDA version. In the following example, we download cuDNN 8.9.7 for CUDA 12.x and install as below: tar xvf cudnn-linux-x86_64-8.9.7.29_cuda12-archive.tar.xz --strip-components=1 -C /home/shahn/.local/cuda-12.8 Finally, set environment variables export CUDA_HOME=/home/shahn/.local/cuda-12.8 export PATH=$CUDA_HOME/bin:$PATH export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$CUDA_HOME/lib:$LD_LIBRARY_PATH and check: which nvcc nvcc --version Then the output should look like this: /home/shahn/.local/cuda-12.8/bin/nvcc nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2025 NVIDIA Corporation Built on Fri_Feb_21_20:23:50_PST_2025 Cuda compilation tools, release 12.8, V12.8.93 Build cuda_12.8.r12.8/compiler.35583870_0 (2) Install PyTorch and Torchaudio Check which torchaudio version matches your PyTorch version at here . Then install approriate version of PyTorch and Torchaudio. In the following example, we install PyTorch 2.7.1 , CUDA 12.8 as below: pip install torch==2.7.1+cu128 torchaudio==2.7.1+cu128 --index-url https://download.pytorch.org/whl (3) Install other dependencies pip install jupyter notebook matplotlib tensorboard scipy librosa unidecode einops cython tqdm pyyaml pesq pystoi torch-pesq torchmetrics","title":"Installation for Training"},{"location":"installation/training/#installation-for-training","text":"We tested under: - PyTorch==2.7.1, CudaToolkit==11.8, Python==3.13 - PyTorch==2.7.1, CudaToolkit==12.8, Python==3.13 Note that we failed under PyTorch==2.8.0. - We succeeded to train, calculate metrics, export ONNX spec2spec, and execute ONNXRuntime. - However, we failed to export ONNX wav2wav. After downgrading to PyTorch==2.7.1, the problem is solved.","title":"Installation for Training"},{"location":"installation/training/#0-decide-python-cuda-toolkit-and-pytorch-versions","text":"Before install, you have to decide which version to install (including Python, CUDA toolkit, and PyTorch). Note that PyTorch>=2.3 is recommended. On PyTorch<2.3 , torch.nn.utils.parametrizations.weight_norm is not implemented, so you have to change the codes and .yaml files. You also have to remove device_id argument of dist.init_process_group in train.py . First, check CUDA toolkit versions that your nvidia driver supports: nvidia-smi | grep \"CUDA Version\" The output should look like this: | NVIDIA-SMI 580.65.06 Driver Version: 580.65.06 CUDA Version: 13.0 | That is the maximum CUDA toolkit version you can install. In our case, we can choose any version <= 13.0 . Second, visit here and decide PyTorch, Python, and CUDA toolkit version. Then install Python to your environment. For the rest of this document, we will use torch-2.7.1+cu128-cp313 version, meaning PyTorch 2.7.1 , CUDA toolkit 12.8 , and Python 3.13 . You can use your favorite environment manager. We use miniconda as below: conda create -n fastenhancer python=3.13 -c conda-forge conda activate fastenhancer","title":"(0) Decide Python, CUDA toolkit, and PyTorch versions"},{"location":"installation/training/#1-install-cuda-toolkit-and-cudnn","text":"Download a local runfile of CUDA toolkit and install. In the following example, we will install CUDA toolkit 12.8 in /home/shahn/.local/cuda-12.8 : wget https://developer.download.nvidia.com/compute/cuda/12.8.1/local_installers/cuda_12.8.1_570.124.06_linux.run chmod +x cuda_12.8.1_570.124.06_linux.run ./cuda_12.8.1_570.124.06_linux.run \\ --silent \\ --toolkit \\ --installpath=/home/shahn/.local/cuda-12.8 \\ --no-opengl-libs \\ --no-drm \\ --no-man-page Then, install a tar file of cuDNN for your CUDA version. In the following example, we download cuDNN 8.9.7 for CUDA 12.x and install as below: tar xvf cudnn-linux-x86_64-8.9.7.29_cuda12-archive.tar.xz --strip-components=1 -C /home/shahn/.local/cuda-12.8 Finally, set environment variables export CUDA_HOME=/home/shahn/.local/cuda-12.8 export PATH=$CUDA_HOME/bin:$PATH export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$CUDA_HOME/lib:$LD_LIBRARY_PATH and check: which nvcc nvcc --version Then the output should look like this: /home/shahn/.local/cuda-12.8/bin/nvcc nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2025 NVIDIA Corporation Built on Fri_Feb_21_20:23:50_PST_2025 Cuda compilation tools, release 12.8, V12.8.93 Build cuda_12.8.r12.8/compiler.35583870_0","title":"(1) Install CUDA toolkit and cuDNN"},{"location":"installation/training/#2-install-pytorch-and-torchaudio","text":"Check which torchaudio version matches your PyTorch version at here . Then install approriate version of PyTorch and Torchaudio. In the following example, we install PyTorch 2.7.1 , CUDA 12.8 as below: pip install torch==2.7.1+cu128 torchaudio==2.7.1+cu128 --index-url https://download.pytorch.org/whl","title":"(2) Install PyTorch and Torchaudio"},{"location":"installation/training/#3-install-other-dependencies","text":"pip install jupyter notebook matplotlib tensorboard scipy librosa unidecode einops cython tqdm pyyaml pesq pystoi torch-pesq torchmetrics","title":"(3) Install other dependencies"},{"location":"train/","text":"Training Before you start training, make sure to prepare datasets . Training code Codes for training is in wrappers/ns.py (for Voicebank-Demand) and wrappers/ns_on_the_fly.py (for DNS-Challenge). Training codes perform the following jobs. 1. For every epoch, they train a model and writes losses to the tensorboard. If train.plot_params_and_grad is set to true, they write parameters and gradients to the tensorboard. 2. For every epoch, they validates and writes losses to the tensorboard. 3. For every train.save_interval epochs, it saves the checkpoints. 4. For every infer.interval epochs, it inferences some samples and writes audios to the tensorboard. 5. For every pesq.interval epochs, it calculates objective metrics (PESQ and STOI) and writes to the tensorboard. You can set those interval s in config/*.yaml files. Training recipes FastEnhancer BSRNN FSPEN LiSenNet Cleaning checkpoints After training, many checkpoints are generated. In most cases, we only need the last one. If you want to remove all the checkpoints except the last one, This section is for you. Suppose the log directory looks like this: logs \u251c\u2500 vbd | \u251c\u2500 fastenhancer_b | | \u251c\u2500 00020.pth | | \u251c\u2500 ... | | \u251c\u2500 00480.pth | | \u2514\u2500 00500.pth | \u2514\u2500 fastenhancer_t | \u251c\u2500 00020.pth | \u251c\u2500 ... | \u251c\u2500 00480.pth | \u2514\u2500 00500.pth \u2514\u2500 dns \u2514\u2500 fastenhancer_b \u251c\u2500 00020.pth \u251c\u2500 ... \u251c\u2500 00480.pth \u2514\u2500 00500.pth If you want to delete all checkpoints except the last one in logs/vbd , run the following code: python scripts/clean_checkpoints.py -n vbd --delete If you just want to check how many checkpoints you can delete, instead of actually deleting them, run without the --delete flag: python scripts/clean_checkpoints.py -n vbd After deleting the checkpoints in logs/vbd , the log directory will be: logs \u251c\u2500 vbd | \u251c\u2500 fastenhancer_b | | \u2514\u2500 00500.pth | \u2514\u2500 fastenhancer_t | \u2514\u2500 00500.pth \u2514\u2500 dns \u2514\u2500 fastenhancer_b \u251c\u2500 00020.pth \u251c\u2500 ... \u251c\u2500 00480.pth \u2514\u2500 00500.pth Experience sharing Except for Voicebank-Demand at 16kHz sampling rate, we recommend not to use PESQLoss. The reasons are: 1. It harms stable training. 2. It doesn't improve other metrics so much (in VoiceBank-Demand @ 16kHz, other metrics marginally improves, so we included it in our paper). 3. The loss includes multiple IIR filter calculations, resulting in increased training time. In our experiments, we found that using MetricGAN instead of PESQLoss shows inferior results. MetricGAN achieved a smaller PESQ improvement than PESQLoss and degraded other objective metrics. However, these results may vary depending on the loss functions, batch size, datasets, and models.","title":"Overview"},{"location":"train/#training","text":"Before you start training, make sure to prepare datasets .","title":"Training"},{"location":"train/#training-code","text":"Codes for training is in wrappers/ns.py (for Voicebank-Demand) and wrappers/ns_on_the_fly.py (for DNS-Challenge). Training codes perform the following jobs. 1. For every epoch, they train a model and writes losses to the tensorboard. If train.plot_params_and_grad is set to true, they write parameters and gradients to the tensorboard. 2. For every epoch, they validates and writes losses to the tensorboard. 3. For every train.save_interval epochs, it saves the checkpoints. 4. For every infer.interval epochs, it inferences some samples and writes audios to the tensorboard. 5. For every pesq.interval epochs, it calculates objective metrics (PESQ and STOI) and writes to the tensorboard. You can set those interval s in config/*.yaml files.","title":"Training code"},{"location":"train/#training-recipes","text":"FastEnhancer BSRNN FSPEN LiSenNet","title":"Training recipes"},{"location":"train/#cleaning-checkpoints","text":"After training, many checkpoints are generated. In most cases, we only need the last one. If you want to remove all the checkpoints except the last one, This section is for you. Suppose the log directory looks like this: logs \u251c\u2500 vbd | \u251c\u2500 fastenhancer_b | | \u251c\u2500 00020.pth | | \u251c\u2500 ... | | \u251c\u2500 00480.pth | | \u2514\u2500 00500.pth | \u2514\u2500 fastenhancer_t | \u251c\u2500 00020.pth | \u251c\u2500 ... | \u251c\u2500 00480.pth | \u2514\u2500 00500.pth \u2514\u2500 dns \u2514\u2500 fastenhancer_b \u251c\u2500 00020.pth \u251c\u2500 ... \u251c\u2500 00480.pth \u2514\u2500 00500.pth If you want to delete all checkpoints except the last one in logs/vbd , run the following code: python scripts/clean_checkpoints.py -n vbd --delete If you just want to check how many checkpoints you can delete, instead of actually deleting them, run without the --delete flag: python scripts/clean_checkpoints.py -n vbd After deleting the checkpoints in logs/vbd , the log directory will be: logs \u251c\u2500 vbd | \u251c\u2500 fastenhancer_b | | \u2514\u2500 00500.pth | \u2514\u2500 fastenhancer_t | \u2514\u2500 00500.pth \u2514\u2500 dns \u2514\u2500 fastenhancer_b \u251c\u2500 00020.pth \u251c\u2500 ... \u251c\u2500 00480.pth \u2514\u2500 00500.pth","title":"Cleaning checkpoints"},{"location":"train/#experience-sharing","text":"Except for Voicebank-Demand at 16kHz sampling rate, we recommend not to use PESQLoss. The reasons are: 1. It harms stable training. 2. It doesn't improve other metrics so much (in VoiceBank-Demand @ 16kHz, other metrics marginally improves, so we included it in our paper). 3. The loss includes multiple IIR filter calculations, resulting in increased training time. In our experiments, we found that using MetricGAN instead of PESQLoss shows inferior results. MetricGAN achieved a smaller PESQ improvement than PESQLoss and degraded other objective metrics. However, these results may vary depending on the loss functions, batch size, datasets, and models.","title":"Experience sharing"},{"location":"train/bsrnn/","text":"BSRNN Original paper that proposed BSRNN: Paper [1] BSRNN applied to noise suppression: Paper [2] BSRNN model size scaling: Paper [3] | Github We implemented a streaming BSRNN with batch normalization. We followed [3] for the configurations of different sizes. [1]: Y. Luo and J. Yu, \u201cMusic source separation with band-split RNN\u201d, IEEE/ACM Trans. ASLP , vol. 31, pp. 1893-1901, 2023. [2]: J. Yu, H. Chen, Y. Luo, R. Gu, and C. Weng, \u201cHigh fidelity speech enhancement with band-split RNN,\u201d in Proc. Interspeech , 2023, pp. 2483\u20132487. [3]: W. Zhang, K. Saijo, J.-w. Jung, C. Li, S. Watanabe, and Y. Qian, \u201cBeyond performance plateaus: A comprehensive study on scalability in speech enhancement,\u201d in Proc. Interspeech , 2024, pp. 1740-1744. Training Model: BSRNN-xxt Dataset: Voicebank-Demand at 16kHz sampling rate. Number of GPUs: 1 Batch size: 64 Mixed-precision training with fp16: False Path to save config, tensorboard logs, and checkpoints: logs/vbd/16khz/bsrnn_xxt CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/bsrnn_xxt \\ -c configs/others/bsrnn_xxt.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/bsrnn_xxt \\ -c configs/others/bsrnn_xxt.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f Options: - -n (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - -c (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - -p (Optional): Parameters after this will update the configuration. - -f (Optional): If the base directory already exists and -c flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file. Resume Training Suppose you stopped the training. To load the saved checkpoint at logs/vbd/16khz/bsrnn_xxt and resume the training, use the code below: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/bsrnn_xxt or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/bsrnn_xxt Test the training code Before you start training, we recommend to run the following test code: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n delete_it \\ -c configs/others/bsrnn_xxt.yaml \\ -p train.test=True pesq.interval=1 \\ -f By setting train.test=True , it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting pesq.interval=1 , it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete logs/delete_it directory after the test. About optimizer_groups In configs/others/bsrnn_xxt.yaml , you may notice that instead of using train.optimizer=AdamP as in FastEnhancer, it uses train.optimizer=AdamW . This is because there's no scale-invariant parameter in BSRNN. It employs pre-activation and no weight norm is applied. In such case, AdamP is same as `AdamW.","title":"BSRNN"},{"location":"train/bsrnn/#bsrnn","text":"Original paper that proposed BSRNN: Paper [1] BSRNN applied to noise suppression: Paper [2] BSRNN model size scaling: Paper [3] | Github We implemented a streaming BSRNN with batch normalization. We followed [3] for the configurations of different sizes. [1]: Y. Luo and J. Yu, \u201cMusic source separation with band-split RNN\u201d, IEEE/ACM Trans. ASLP , vol. 31, pp. 1893-1901, 2023. [2]: J. Yu, H. Chen, Y. Luo, R. Gu, and C. Weng, \u201cHigh fidelity speech enhancement with band-split RNN,\u201d in Proc. Interspeech , 2023, pp. 2483\u20132487. [3]: W. Zhang, K. Saijo, J.-w. Jung, C. Li, S. Watanabe, and Y. Qian, \u201cBeyond performance plateaus: A comprehensive study on scalability in speech enhancement,\u201d in Proc. Interspeech , 2024, pp. 1740-1744.","title":"BSRNN"},{"location":"train/bsrnn/#training","text":"Model: BSRNN-xxt Dataset: Voicebank-Demand at 16kHz sampling rate. Number of GPUs: 1 Batch size: 64 Mixed-precision training with fp16: False Path to save config, tensorboard logs, and checkpoints: logs/vbd/16khz/bsrnn_xxt CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/bsrnn_xxt \\ -c configs/others/bsrnn_xxt.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/bsrnn_xxt \\ -c configs/others/bsrnn_xxt.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f Options: - -n (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - -c (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - -p (Optional): Parameters after this will update the configuration. - -f (Optional): If the base directory already exists and -c flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file.","title":"Training"},{"location":"train/bsrnn/#resume-training","text":"Suppose you stopped the training. To load the saved checkpoint at logs/vbd/16khz/bsrnn_xxt and resume the training, use the code below: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/bsrnn_xxt or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/bsrnn_xxt","title":"Resume Training"},{"location":"train/bsrnn/#test-the-training-code","text":"Before you start training, we recommend to run the following test code: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n delete_it \\ -c configs/others/bsrnn_xxt.yaml \\ -p train.test=True pesq.interval=1 \\ -f By setting train.test=True , it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting pesq.interval=1 , it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete logs/delete_it directory after the test.","title":"Test the training code"},{"location":"train/bsrnn/#about-optimizer_groups","text":"In configs/others/bsrnn_xxt.yaml , you may notice that instead of using train.optimizer=AdamP as in FastEnhancer, it uses train.optimizer=AdamW . This is because there's no scale-invariant parameter in BSRNN. It employs pre-activation and no weight norm is applied. In such case, AdamP is same as `AdamW.","title":"About optimizer_groups"},{"location":"train/fastenhancer/","text":"FastEnhancer Paper [1] | Github [1] S. Ahn, J. Han, B. J. Woo, and N. S. Kim, \u201cFastEnhancer: Speed-optimized streaming neural speech enhancement,\u201d, arXiv:2509.21867 , 2025. Training FastEnhancer-Large on Voicebank-Demand 16kHz Model: FastEnhancer-Large Dataset: Voicebank-Demand at 16kHz sampling rate Number of GPUs: 4 Batch size: 16/GPU, total 64 Mixed-precision training with fp16: True Path to save config, tensorboard logs, and checkpoints: logs/vbd/16khz/fastenhancer_l CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py \\ -n vbd/16khz/fastenhancer_l \\ -c configs/fastenhancer/l.yaml \\ -p train.batch_size=16 valid.batch_size=16 pesq.batch_size=4 \\ -f or CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --standalone --nproc_per_node=4 \\ train_torchrun.py \\ -n vbd/16khz/fastenhancer_l \\ -c configs/fastenhancer/l.yaml \\ -p train.batch_size=16 valid.batch_size=16 pesq.batch_size=4 \\ -f Training FastEnhancer-Huge on DNS-Challenge 16kHz Model: FastEnhancer-Huge-Noncausal Dataset: DNS-Challenge at 16kHz sampling rate Number of GPUs: 4 Batch size: 16/GPU, total 64 Mixed-precision training with fp16: True Path to save config, tensorboard logs, and checkpoints: logs/dns/16khz/fastenhancer_h CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py \\ -n dns/16khz/fastenhancer_h \\ -c configs/fastenhancer/huge_noncausal_dns.yaml \\ -p train.batch_size=16 valid.batch_size=16 pesq.batch_size=4 \\ -f Options: - -n (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - -c (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - -p (Optional): Parameters after this will update the configuration. - -f (Optional): If the base directory already exists and -c flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file. Resume Training Suppose you stopped the training. To load the saved checkpoint at logs/vbd/16khz/fastenhancer_l and resume the training, use the code below: CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py \\ -n vbd/16khz/fastenhancer_l or CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --standalone --nproc_per_node=4 \\ train_torchrun.py \\ -n vbd/16khz/fastenhancer_l Test the training code Before you start training, we recommend to run the following test code: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n delete_it \\ -c configs/fastenhancer/l.yaml \\ -p train.test=True pesq.interval=1 \\ -f By setting train.test=True , it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting pesq.interval=1 , it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete logs/delete_it directory after the test. About optimizer_groups In configs/fastenhancer/l.yaml , you can see a large code block for train.optimizer_groups . This section is for ones who want to know the details of those lines. The lines below set weight_decay=0 and projection=disabled for weight_g s of GRUs and scale parameter of the final convolution of the decoder. If we apply weight_norm to GRUs, we empirically found that setting weight_decay=0 to weight_g s improves performance. We believe this is because of tanh and sigmoid functions applied after weight-input multiplication. Also, intuitively, we should not apply weight_decay to the final mask prediction. - regex_list : - \"rf_block\\\\.\\\\d\\\\.rnn\\\\.parametrizations.+original0$\" # GRU weight_g - \"dec_post\\\\.3\\\\.scale\" # scale parameter of the final conv weight_decay : 0 projection : disabled The lines below set projection=channelwise and projection=layerwise to appropriate parameters. Note that originally AdamP(projection=auto) can automatically handle them by detecting scale-invariant parameters. However, we found that when using mixed-precision training (by setting train.fp16=True ), AdamP often failed to detect scale-invariance because of the numerical error. Therefore, we manually set projection s. - regex_list : - \".+parametrizations.+original1$\" # weight_v - \"enc_pre\\\\.0\\\\.weight\" # conv1d before BN projection : channelwise - regex_list : - \"dec_post\\\\.3\\\\.weight\" # final conv projection : layerwise","title":"FastEnhancer"},{"location":"train/fastenhancer/#fastenhancer","text":"Paper [1] | Github [1] S. Ahn, J. Han, B. J. Woo, and N. S. Kim, \u201cFastEnhancer: Speed-optimized streaming neural speech enhancement,\u201d, arXiv:2509.21867 , 2025.","title":"FastEnhancer"},{"location":"train/fastenhancer/#training-fastenhancer-large-on-voicebank-demand-16khz","text":"Model: FastEnhancer-Large Dataset: Voicebank-Demand at 16kHz sampling rate Number of GPUs: 4 Batch size: 16/GPU, total 64 Mixed-precision training with fp16: True Path to save config, tensorboard logs, and checkpoints: logs/vbd/16khz/fastenhancer_l CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py \\ -n vbd/16khz/fastenhancer_l \\ -c configs/fastenhancer/l.yaml \\ -p train.batch_size=16 valid.batch_size=16 pesq.batch_size=4 \\ -f or CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --standalone --nproc_per_node=4 \\ train_torchrun.py \\ -n vbd/16khz/fastenhancer_l \\ -c configs/fastenhancer/l.yaml \\ -p train.batch_size=16 valid.batch_size=16 pesq.batch_size=4 \\ -f","title":"Training FastEnhancer-Large on Voicebank-Demand 16kHz"},{"location":"train/fastenhancer/#training-fastenhancer-huge-on-dns-challenge-16khz","text":"Model: FastEnhancer-Huge-Noncausal Dataset: DNS-Challenge at 16kHz sampling rate Number of GPUs: 4 Batch size: 16/GPU, total 64 Mixed-precision training with fp16: True Path to save config, tensorboard logs, and checkpoints: logs/dns/16khz/fastenhancer_h CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py \\ -n dns/16khz/fastenhancer_h \\ -c configs/fastenhancer/huge_noncausal_dns.yaml \\ -p train.batch_size=16 valid.batch_size=16 pesq.batch_size=4 \\ -f Options: - -n (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - -c (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - -p (Optional): Parameters after this will update the configuration. - -f (Optional): If the base directory already exists and -c flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file.","title":"Training FastEnhancer-Huge on DNS-Challenge 16kHz"},{"location":"train/fastenhancer/#resume-training","text":"Suppose you stopped the training. To load the saved checkpoint at logs/vbd/16khz/fastenhancer_l and resume the training, use the code below: CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py \\ -n vbd/16khz/fastenhancer_l or CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --standalone --nproc_per_node=4 \\ train_torchrun.py \\ -n vbd/16khz/fastenhancer_l","title":"Resume Training"},{"location":"train/fastenhancer/#test-the-training-code","text":"Before you start training, we recommend to run the following test code: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n delete_it \\ -c configs/fastenhancer/l.yaml \\ -p train.test=True pesq.interval=1 \\ -f By setting train.test=True , it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting pesq.interval=1 , it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete logs/delete_it directory after the test.","title":"Test the training code"},{"location":"train/fastenhancer/#about-optimizer_groups","text":"In configs/fastenhancer/l.yaml , you can see a large code block for train.optimizer_groups . This section is for ones who want to know the details of those lines. The lines below set weight_decay=0 and projection=disabled for weight_g s of GRUs and scale parameter of the final convolution of the decoder. If we apply weight_norm to GRUs, we empirically found that setting weight_decay=0 to weight_g s improves performance. We believe this is because of tanh and sigmoid functions applied after weight-input multiplication. Also, intuitively, we should not apply weight_decay to the final mask prediction. - regex_list : - \"rf_block\\\\.\\\\d\\\\.rnn\\\\.parametrizations.+original0$\" # GRU weight_g - \"dec_post\\\\.3\\\\.scale\" # scale parameter of the final conv weight_decay : 0 projection : disabled The lines below set projection=channelwise and projection=layerwise to appropriate parameters. Note that originally AdamP(projection=auto) can automatically handle them by detecting scale-invariant parameters. However, we found that when using mixed-precision training (by setting train.fp16=True ), AdamP often failed to detect scale-invariance because of the numerical error. Therefore, we manually set projection s. - regex_list : - \".+parametrizations.+original1$\" # weight_v - \"enc_pre\\\\.0\\\\.weight\" # conv1d before BN projection : channelwise - regex_list : - \"dec_post\\\\.3\\\\.weight\" # final conv projection : layerwise","title":"About optimizer_groups"},{"location":"train/fspen/","text":"FSPEN Paper [1] Since there's no official implementation, we faithfully re-implemented the model architecture following the paper and configured the training settings identically to FastEnhancer for fair comparison. [1]: L. Yang, W. Liu, R. Meng, G. Lee, S. Baek, and H.-G. Moon, \u201cFspen: an ultra-lightweight network for real time speech enahncment,\u201d in Proc. IEEE ICASSP , 2024, pp. 10671\u201310675. Training Model: FSPEN Dataset: Voicebank-Demand at 16kHz sampling rate. Number of GPUs: 1 Batch size: 64 Mixed-precision training with fp16: False Path to save config, tensorboard logs, and checkpoints: logs/vbd/16khz/fspen CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/fspen \\ -c configs/others/fspen.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/fspen \\ -c configs/others/fspen.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f Options: - -n (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - -c (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - -p (Optional): Parameters after this will update the configuration. - -f (Optional): If the base directory already exists and -c flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file. Resume Training Suppose you stopped the training. To load the saved checkpoint at logs/vbd/16khz/fspen and resume the training, use the code below: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/fspen or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/fspen Test the training code Before you start training, we recommend to run the following test code: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n delete_it \\ -c configs/others/fspen.yaml \\ -p train.test=True pesq.interval=1 \\ -f By setting train.test=True , it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting pesq.interval=1 , it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete logs/delete_it directory after the test. About optimizer_groups In configs/fastenhancer/l.yaml , take a look at train.optimizer_groups . There are only two scale-invariant parameters in FSPEN, and we manually set them.","title":"FSPEN"},{"location":"train/fspen/#fspen","text":"Paper [1] Since there's no official implementation, we faithfully re-implemented the model architecture following the paper and configured the training settings identically to FastEnhancer for fair comparison. [1]: L. Yang, W. Liu, R. Meng, G. Lee, S. Baek, and H.-G. Moon, \u201cFspen: an ultra-lightweight network for real time speech enahncment,\u201d in Proc. IEEE ICASSP , 2024, pp. 10671\u201310675.","title":"FSPEN"},{"location":"train/fspen/#training","text":"Model: FSPEN Dataset: Voicebank-Demand at 16kHz sampling rate. Number of GPUs: 1 Batch size: 64 Mixed-precision training with fp16: False Path to save config, tensorboard logs, and checkpoints: logs/vbd/16khz/fspen CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/fspen \\ -c configs/others/fspen.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/fspen \\ -c configs/others/fspen.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f Options: - -n (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - -c (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - -p (Optional): Parameters after this will update the configuration. - -f (Optional): If the base directory already exists and -c flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file.","title":"Training"},{"location":"train/fspen/#resume-training","text":"Suppose you stopped the training. To load the saved checkpoint at logs/vbd/16khz/fspen and resume the training, use the code below: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/fspen or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/fspen","title":"Resume Training"},{"location":"train/fspen/#test-the-training-code","text":"Before you start training, we recommend to run the following test code: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n delete_it \\ -c configs/others/fspen.yaml \\ -p train.test=True pesq.interval=1 \\ -f By setting train.test=True , it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting pesq.interval=1 , it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete logs/delete_it directory after the test.","title":"Test the training code"},{"location":"train/fspen/#about-optimizer_groups","text":"In configs/fastenhancer/l.yaml , take a look at train.optimizer_groups . There are only two scale-invariant parameters in FSPEN, and we manually set them.","title":"About optimizer_groups"},{"location":"train/lisennet/","text":"LiSenNet Paper [1] | Github The official implementation includes input normalization and Griffin-Lim, so it is not streamable. To make the model streamable, input normalization is removed, and instead of Griffin-Lim, the model predicts a complex mask. We configured the training settings identically to FastEnhancer for fair comparison. [1]: H. Yan, J. Zhang, C. Fan, Y. Zhou, and P. Liu, \u201cLiSenNet: Lightweight sub-band and dual-path modeling for real-time speech enhancement,\u201d in Proc. IEEE ICASSP , 2025, pp. 1\u20135. Training Model: LiSenNet Dataset: Voicebank-Demand at 16kHz sampling rate. Number of GPUs: 1 Batch size: 64 Mixed-precision training with fp16: False Path to save config, tensorboard logs, and checkpoints: logs/vbd/16khz/lisennet CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/lisennet \\ -c configs/others/lisennet.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/lisennet \\ -c configs/others/lisennet.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f Options: - -n (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - -c (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - -p (Optional): Parameters after this will update the configuration. - -f (Optional): If the base directory already exists and -c flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file. Resume Training Suppose you stopped the training. To load the saved checkpoint at logs/vbd/16khz/lisennet and resume the training, use the code below: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/lisennet or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/lisennet Test the training code Before you start training, we recommend to run the following test code: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n delete_it \\ -c configs/others/lisennet.yaml \\ -p train.test=True pesq.interval=1 \\ -f By setting train.test=True , it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting pesq.interval=1 , it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete logs/delete_it directory after the test. About Optimizer In configs/others/lisennet.yaml , you may notice that instead of using train.optimizer=AdamP as in FastEnhancer, it uses train.optimizer=AdamW . This is because there's no scale-invariant parameter in LiSenNet. It employs LayerNorm and no weight norm is applied. In such case, AdamP is same as AdamW. For PReLU weights, one should not apply weight decay, so we set train.optimizer_groups appropriately.","title":"LiSenNet"},{"location":"train/lisennet/#lisennet","text":"Paper [1] | Github The official implementation includes input normalization and Griffin-Lim, so it is not streamable. To make the model streamable, input normalization is removed, and instead of Griffin-Lim, the model predicts a complex mask. We configured the training settings identically to FastEnhancer for fair comparison. [1]: H. Yan, J. Zhang, C. Fan, Y. Zhou, and P. Liu, \u201cLiSenNet: Lightweight sub-band and dual-path modeling for real-time speech enhancement,\u201d in Proc. IEEE ICASSP , 2025, pp. 1\u20135.","title":"LiSenNet"},{"location":"train/lisennet/#training","text":"Model: LiSenNet Dataset: Voicebank-Demand at 16kHz sampling rate. Number of GPUs: 1 Batch size: 64 Mixed-precision training with fp16: False Path to save config, tensorboard logs, and checkpoints: logs/vbd/16khz/lisennet CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/lisennet \\ -c configs/others/lisennet.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/lisennet \\ -c configs/others/lisennet.yaml \\ -p train.batch_size=64 valid.batch_size=64 \\ -f Options: - -n (Required): Base directory to save configuration, tensorboard logs, and checkpoints. - -c (Optional): Path to configuration file. If not given, the configuration file in the base directory will be used. - -p (Optional): Parameters after this will update the configuration. - -f (Optional): If the base directory already exists and -c flag is given, an exception will be raised to avoid overwriting config file. However, enabling this option will force overwriting config file.","title":"Training"},{"location":"train/lisennet/#resume-training","text":"Suppose you stopped the training. To load the saved checkpoint at logs/vbd/16khz/lisennet and resume the training, use the code below: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n vbd/16khz/lisennet or CUDA_VISIBLE_DEVICES=0 torchrun --standalone --nproc_per_node=1 \\ train_torchrun.py \\ -n vbd/16khz/lisennet","title":"Resume Training"},{"location":"train/lisennet/#test-the-training-code","text":"Before you start training, we recommend to run the following test code: CUDA_VISIBLE_DEVICES=0 python train.py \\ -n delete_it \\ -c configs/others/lisennet.yaml \\ -p train.test=True pesq.interval=1 \\ -f By setting train.test=True , it will execute a training code for only 10 steps. Then it will execute a validation code. Finally, by setting pesq.interval=1 , it will execute a code for calculating objective metrics and terminate. If the code runs well, you are ready to begin training. You can delete logs/delete_it directory after the test.","title":"Test the training code"},{"location":"train/lisennet/#about-optimizer","text":"In configs/others/lisennet.yaml , you may notice that instead of using train.optimizer=AdamP as in FastEnhancer, it uses train.optimizer=AdamW . This is because there's no scale-invariant parameter in LiSenNet. It employs LayerNorm and no weight norm is applied. In such case, AdamP is same as AdamW. For PReLU weights, one should not apply weight decay, so we set train.optimizer_groups appropriately.","title":"About Optimizer"}]}